{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLHjtMPDhu2K"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers, models, Model\n",
        "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import pickle\n",
        "import os\n",
        "import librosa\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoFeatureExtractor, Wav2Vec2FeatureExtractor, TFWav2Vec2Model, Wav2Vec2Processor\n",
        "\n",
        "# Only log error messages\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "# Set random seed\n",
        "tf.keras.utils.set_random_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/bin/bash\n",
        "!curl -L -o ./deep-voice-deepfake-voice-recognition.zip\\\n",
        "  https://www.kaggle.com/api/v1/datasets/download/birdy654/deep-voice-deepfake-voice-recognition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlsUAxvyjFjl",
        "outputId": "1e5758ba-df0e-49fa-b817-e35c8ff08afd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 3773M  100 3773M    0     0  38.7M      0  0:01:37  0:01:37 --:--:-- 39.7M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: unzip the file\n",
        "\n",
        "!unzip ./deep-voice-deepfake-voice-recognition.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52sWunLZkLTj",
        "outputId": "2e509aa2-c207-4245-feb9-900b9bc5186d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ./deep-voice-deepfake-voice-recognition.zip\n",
            "  inflating: DEMONSTRATION/DEMONSTRATION/linus-original-DEMO.mp3  \n",
            "  inflating: DEMONSTRATION/DEMONSTRATION/linus-to-musk-DEMO.mp3  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/Obama-to-Biden.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/Obama-to-Trump.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/biden-to-Obama.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/biden-to-Trump.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/biden-to-linus.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/biden-to-margot.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/biden-to-musk.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/biden-to-ryan.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/biden-to-taylor.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/linus-to-biden.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/linus-to-margot.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/linus-to-musk.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/linus-to-obama.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/linus-to-ryan.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/linus-to-taylor.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/linus-to-trump.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/margot-to-biden.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/margot-to-linus.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/margot-to-musk.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/margot-to-obama.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/margot-to-ryan.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/margot-to-taylor.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/margot-to-trump.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/musk-to-biden.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/musk-to-linus.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/musk-to-margot.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/musk-to-obama.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/musk-to-ryan.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/musk-to-taylor.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/musk-to-trump.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/obama-to-linus.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/obama-to-margot.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/obama-to-musk.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/obama-to-ryan.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/obama-to-taylor.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/ryan-to-biden.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/ryan-to-linus.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/ryan-to-margot.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/ryan-to-musk.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/ryan-to-obama.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/ryan-to-taylor.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/ryan-to-trump.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/taylor-to-biden.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/taylor-to-linus.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/taylor-to-margot.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/taylor-to-musk.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/taylor-to-obama.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/taylor-to-ryan.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/taylor-to-trump.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/trump-to-Biden.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/trump-to-Obama.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/trump-to-linus.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/trump-to-margot.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/trump-to-musk.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/trump-to-ryan.wav  \n",
            "  inflating: KAGGLE/AUDIO/FAKE/trump-to-taylor.wav  \n",
            "  inflating: KAGGLE/AUDIO/REAL/biden-original.wav  \n",
            "  inflating: KAGGLE/AUDIO/REAL/linus-original.wav  \n",
            "  inflating: KAGGLE/AUDIO/REAL/margot-original.wav  \n",
            "  inflating: KAGGLE/AUDIO/REAL/musk-original.wav  \n",
            "  inflating: KAGGLE/AUDIO/REAL/obama-original.wav  \n",
            "  inflating: KAGGLE/AUDIO/REAL/ryan-original.wav  \n",
            "  inflating: KAGGLE/AUDIO/REAL/taylor-original.wav  \n",
            "  inflating: KAGGLE/AUDIO/REAL/trump-original.wav  \n",
            "  inflating: KAGGLE/DATASET-balanced.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vESh7G0Ohu2L"
      },
      "source": [
        "### Basic configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2fN66xphu2L"
      },
      "outputs": [],
      "source": [
        "# Maximum duration of the input audio file we feed to our Wav2Vec 2.0 model.\n",
        "MAX_DURATION = 5  # Change to 30 seconds\n",
        "\n",
        "# Sampling rate is the number of samples of audio recorded every second\n",
        "SAMPLING_RATE = 16000\n",
        "BATCH_SIZE = 8  # Batch-size for training and evaluating our model.\n",
        "NUM_CLASSES = 2  # Number of classes our dataset will have (2 in our case).\n",
        "HIDDEN_DIM = 768  # Dimension of our model output (768 in case of Wav2Vec 2.0 - Base).\n",
        "MAX_SEQ_LENGTH = MAX_DURATION * SAMPLING_RATE  # Maximum length of the input audio file.\n",
        "\n",
        "# Wav2Vec 2.0 results in an output frequency with a stride of about 20ms.\n",
        "MAX_FRAMES = 249 # Adjust for 30 seconds\n",
        "MAX_EPOCHS = 5  # Maximum number of training epochs.\n",
        "SEED = 42\n",
        "MODEL_CHECKPOINT = \"facebook/wav2vec2-base\"  # Name of pretrained model from Hugging Face Model Hu\n",
        "\n",
        "FAKE = \"KAGGLE/AUDIO/FAKE\"\n",
        "REAL = \"KAGGLE/AUDIO/REAL\"\n",
        "\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")  #retrieve feature extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRo7-4Ophu2M"
      },
      "source": [
        "### Load files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "454pHtL7hu2M"
      },
      "outputs": [],
      "source": [
        "def load_audio_files_and_labels(fake_folder, real_folder):\n",
        "    audio_data = []\n",
        "    labels = []\n",
        "\n",
        "    # Load fake audio files\n",
        "    for filename in os.listdir(fake_folder):\n",
        "        file_path = os.path.join(fake_folder, filename)\n",
        "        audio, sr = librosa.load(file_path, sr=16000, duration=MAX_DURATION)  # Load 30 seconds\n",
        "        audio_data.append(audio)\n",
        "        labels.append(1)\n",
        "\n",
        "    # Load real audio files\n",
        "    for filename in os.listdir(real_folder):\n",
        "        file_path = os.path.join(real_folder, filename)\n",
        "        audio, sr = librosa.load(file_path, sr=16000, duration=MAX_DURATION)  # Load 30 seconds\n",
        "        audio_data.append(audio)\n",
        "        labels.append(0)\n",
        "\n",
        "    return audio_data, np.array(labels)\n",
        "\n",
        "def extract_features(audio_data):\n",
        "    features = []\n",
        "    for audio in audio_data:\n",
        "        inputs = feature_extractor(audio, sampling_rate=16000, return_tensors=\"np\", padding=True, truncation=True, max_length=MAX_SEQ_LENGTH)\n",
        "        features.append(inputs.input_values)\n",
        "    return np.concatenate(features, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1plVH7ryhu2N",
        "outputId": "ba11a355-c73b-42ff-c300-17f1001597d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 80000)\n"
          ]
        }
      ],
      "source": [
        "audio_data, labels = load_audio_files_and_labels(FAKE, REAL)\n",
        "features = extract_features(audio_data)\n",
        "print(features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRJHmHNuhu2N"
      },
      "source": [
        "### Create training and testing batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9oMZxN0hu2O"
      },
      "outputs": [],
      "source": [
        "X_tr, X_te, y_tr, y_te = train_test_split(features, labels, test_size=0.2, stratify=labels, random_state=SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hEwjV7whu2O"
      },
      "source": [
        "### Create custom Model using TFWav2Vec2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jghALAtahu2O",
        "outputId": "c80ab3ac-57a1-4b2c-8f69-2f6b40e4c6bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:312: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "\n",
            "TFWav2Vec2Model has backpropagation operations that are NOT supported on CPU. If you wish to train/fine-tune this model, you need a GPU or a TPU\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFWav2Vec2Model: ['quantizer.weight_proj.bias', 'project_q.weight', 'project_hid.bias', 'quantizer.weight_proj.weight', 'quantizer.codevectors', 'project_hid.weight', 'project_q.bias']\n",
            "- This IS expected if you are initializing TFWav2Vec2Model from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFWav2Vec2Model from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFWav2Vec2Model were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFWav2Vec2Model for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def mean_pool(hidden_states, feature_lengths, batch_size):\n",
        "    attenion_mask = tf.sequence_mask(\n",
        "        feature_lengths, maxlen=MAX_FRAMES, dtype=tf.dtypes.int64\n",
        "    )\n",
        "    padding_mask = tf.cast(\n",
        "        tf.reverse(tf.cumsum(tf.reverse(attenion_mask, [-1]), -1), [-1]),\n",
        "        dtype=tf.dtypes.bool,\n",
        "    )\n",
        "    hidden_states = tf.where(\n",
        "        tf.broadcast_to(\n",
        "            tf.expand_dims(~padding_mask, -1), (batch_size, MAX_FRAMES, HIDDEN_DIM)\n",
        "        ),\n",
        "        0.0,\n",
        "        hidden_states,\n",
        "    )\n",
        "    pooled_state = tf.math.reduce_sum(hidden_states, axis=1) / tf.reshape(\n",
        "        tf.math.reduce_sum(tf.cast(padding_mask, dtype=tf.dtypes.float32), axis=1),\n",
        "        [-1, 1],\n",
        "    )\n",
        "    return pooled_state\n",
        "\n",
        "class Wav2Vec2_Model(layers.Layer):\n",
        "\n",
        "    def __init__(self, model_checkpoint, num_classes):\n",
        "        super().__init__()\n",
        "        self.wav2vec2 = TFWav2Vec2Model.from_pretrained(\n",
        "            model_checkpoint, apply_spec_augment=False, from_pt=True\n",
        "        )\n",
        "        self.pooling = layers.GlobalAveragePooling1D()\n",
        "        self.intermediate_layer_dropout = layers.Dropout(0.5)\n",
        "        self.final_layer = layers.Dense(num_classes, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        hidden_states = self.wav2vec2(inputs[\"input_values\"])[0]\n",
        "        batch_size = tf.shape(hidden_states)[0]\n",
        "\n",
        "        if tf.is_tensor(inputs[\"attention_mask\"]):\n",
        "            audio_lengths = tf.cumsum(inputs[\"attention_mask\"], -1)[:, -1]\n",
        "            feature_lengths = self.wav2vec2.wav2vec2._get_feat_extract_output_lengths(\n",
        "                audio_lengths\n",
        "            )\n",
        "            pooled_state = mean_pool(hidden_states, feature_lengths, batch_size)\n",
        "        else:\n",
        "            pooled_state = self.pooling(hidden_states)\n",
        "\n",
        "        intermediate_state = self.intermediate_layer_dropout(pooled_state)\n",
        "        final_state = self.final_layer(intermediate_state)\n",
        "\n",
        "        return final_state\n",
        "\n",
        "# Rebuild the model to apply the mixed precision policy\n",
        "def build_model():\n",
        "    inputs = {\n",
        "        \"input_values\": tf.keras.Input(shape=(MAX_SEQ_LENGTH,), dtype=\"float32\"),\n",
        "        \"attention_mask\": tf.keras.Input(shape=(MAX_SEQ_LENGTH,), dtype=\"int32\"),\n",
        "    }\n",
        "    wav2vec2_model = Wav2Vec2_Model(MODEL_CHECKPOINT, NUM_CLASSES)(\n",
        "        inputs\n",
        "    )\n",
        "    model = tf.keras.Model(inputs, wav2vec2_model)\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "model = build_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBFmg5grhu2P",
        "outputId": "7b9d9ab8-7c30-48b2-ed33-3784d17847af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "7/7 [==============================] - 59s 2s/step - loss: 0.3908 - accuracy: 0.8627 - val_loss: 0.4153 - val_accuracy: 0.8462\n",
            "Epoch 2/5\n",
            "7/7 [==============================] - 5s 700ms/step - loss: 0.3308 - accuracy: 0.8824 - val_loss: 0.3781 - val_accuracy: 0.8462\n",
            "Epoch 3/5\n",
            "7/7 [==============================] - 5s 708ms/step - loss: 0.2875 - accuracy: 0.8824 - val_loss: 0.3268 - val_accuracy: 0.8462\n",
            "Epoch 4/5\n",
            "7/7 [==============================] - 5s 704ms/step - loss: 0.2571 - accuracy: 0.8824 - val_loss: 0.2728 - val_accuracy: 0.8462\n",
            "Epoch 5/5\n",
            "7/7 [==============================] - 5s 719ms/step - loss: 0.2150 - accuracy: 0.9216 - val_loss: 0.2181 - val_accuracy: 0.8462\n"
          ]
        }
      ],
      "source": [
        "inputs = tf.data.Dataset.from_tensor_slices(({\"input_values\": X_tr, \"attention_mask\": np.ones_like(X_tr)}, y_tr)).batch(BATCH_SIZE)\n",
        "val_data = tf.data.Dataset.from_tensor_slices(({\"input_values\": X_te, \"attention_mask\": np.ones_like(X_te)}, y_te)).batch(BATCH_SIZE)\n",
        "\n",
        "# Training\n",
        "history = model.fit(\n",
        "    inputs,\n",
        "    validation_data=val_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=MAX_EPOCHS\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfNcYDuZhu2P"
      },
      "source": [
        "### Measure model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJUxmTH5hu2P",
        "outputId": "ef61eb94-cbd4-4f3a-977b-f0d35be35fb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy List:  [0.8627451062202454, 0.8823529481887817, 0.8823529481887817, 0.8823529481887817, 0.9215686321258545]\n",
            "2/2 [==============================] - 5s 162ms/step\n",
            "Accuracy: 0.8461538461538461\n",
            "Precision: 0.8461538461538461\n",
            "Recall: 1.0\n",
            "F1-score: 0.9166666666666666\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Get training accuracy list\n",
        "training_accuracy = history.history['accuracy']\n",
        "print(\"Training Accuracy List: \", training_accuracy)\n",
        "\n",
        "\n",
        "# Get predictions\n",
        "y_pred_probs = model.predict(\n",
        "    {\"input_values\": X_te, \"attention_mask\": np.ones_like(X_te)},\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Convert predicted probabilities to class labels\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_te, y_pred)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(y_te, y_pred, average='binary')\n",
        "recall = recall_score(y_te, y_pred, average='binary')\n",
        "f1 = f1_score(y_te, y_pred, average='binary')\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2p0zFVZmngtq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}